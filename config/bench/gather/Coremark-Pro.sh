#!/bin/bash
#Be aware that LAVA likes TEST_CASE_ID (the first parameter to ltc) to have no
#spaces. ltc passes the first hurdle for handling spaces, but the LAVA UI seems
#to truncate at the first space. And who knows what might be getting mixed up
#elsewhere in the system if we put spaces in the ID.

set -eu
set -o pipefail

declare -A verificationCount
declare -A performanceCount
declare -A medianCount

error=1

function ltc {
  #Funky quoting to help out syntax highlighters
  ${TESTING:+echo} 'lava-test-case' "$@"
}

function ltra {
  #Funky quoting to help out syntax highlighters
  ${TESTING:+echo} 'lava-test-run-attach' "$@"
}

function exit_handler {
  exit ${error}
}

#This only works in some cases, keeping it as it is useful when it works,
#not wasting any more time trying to understand it.
#One fail case is:
#function ename { name; }
#ename
#This exits, but produces no stacktrace.
function err_handler {
  exec 1>&2
  echo "ERROR ${error}"
  echo "Stack trace, excluding subshells:"
  local frame=0
  while caller ${frame}; do
    frame=$((frame + 1))
  done
}

trap exit_handler EXIT
trap err_handler ERR

function check_dir {
  if ! test -d "$1"; then
    echo "'$1' does not exist, or is not a directory" >&2
    exit 1
  fi
}

function check_file {
  if ! test -f "$1"; then
    echo "'$1' does not exist, or is not a file" >&2
    exit 1
  fi
}

function header {
  test x"$1" = xUID
}

function comment {
  test "${1:0:1}" = '#'
}

function _xcmd_verification {
  #For non-cert runs, we rely on Coremark-Pro printing XCMD as the last argument
  #on lines that begin #Results. Its parameters are space-separated with no escaping or
  #quoting. So $7 will be XCMD=<thing> and any further parameters are also part
  #of XCMD. One of them may be -v1.
  #We make this code pretty twitchy -- better to fail to report than to report
  #garbage.
  #Caller must already have checked that line begins with #Results.
  local x v first
  v=''
  shift 6
  if test -z "${1:-}" || test x"${1:0:5}" != 'xXCMD='; then
    echo "XCMD= not at expected location." >&2
    exit 1
  fi

  first="${1:5}"
  shift
  for x in "${first}" "$@"; do
    if test x"${x:0:2}" = 'x-v'; then
      if test -z "${v}"; then
        v="v${x:2}"
      else
        echo "Multiple -v in XCMD. Don't know what this means." >&2
        exit 1
      fi
    fi
  done
  if test -z "${v:-}" || test x"${v}" = xv1; then
    return 0
  elif test x"${v}" = xv0; then
    return 1
  else
    echo "Unknown -v argument '${v:1}'." >&2
    exit 1
  fi
}

function _results {
  test x"$1" = x'#Results'
}

function verification {
  #Can't be a performance line if it is not a results line
  _results "$@" || return 1

  #Certification runs print convenient text
  if test x"$3" = xverification; then
    return 0
  elif test x"$3" = xperformance; then
    return 1
  else #Starts with #Results but not a certification run, therefore this line
       #should be in a format that _xcmd_verification can parse
    _xcmd_verification "$@"
  fi
}

function performance {
  #Can't be a performance line if it is not a results line
  _results "$@" || return 1

  #Certification runs print convenient text
  if test x"$3" = xperformance; then
    return 0
  elif test x"$3" = xverification; then
    return 1
  else #Starts with #Results but not a certification run, therefore this line
       #should be in a format that _xcmd_verification can parse
    ! _xcmd_verification "$@"
  fi
}

function median {
  test x"$1" = x'#Median'
}

function pass {
  test $6 -eq 0
}

function name {
  echo "$3" | tr ' ' _
}

function runtime {
  echo $7
}

function it_per_sec {
  echo $9
}

function code_size {
  shift
  echo $9
}

function data_size {
  shift 2
  echo $9
}

function report_measured {
  local name it runtime it_ps
  name="$1"
  it="$2"
  shift 2
  if pass "$@"; then
    runtime="`runtime $@`"
    it_p_s="`it_per_sec $@`"
    ltc "${name}[${it}]:time" --result pass --units seconds --measurement "${runtime}"
    ltc "${name}[${it}]:rate" --result pass --units "it/s" --measurement "${it_p_s}"
  else
    ltc "${name}[${it}]" --result fail
  fi
}

function marks_name {
  echo "$1" | tr ' ' _
}

function marks_score {
  echo $2
}

function marks_units {
  echo $3
}

function report_marks {
  local markslog="$1"
  check_file "${markslog}"
  if test `wc -l ${markslog} | cut -d ' ' -f 1` -eq 2; then
    line="`sed -n 2p ${markslog} | tr , ' '`"
    ltc "`marks_name ${line}`" --result pass --units "`marks_units ${line}`" \
        --measurement "`marks_score ${line}`"
  else
    echo "Wrong number of lines in marks file" >&2
    false
  fi
}

#Metadata

#Simplifying assumptions:
#All numbers in the log come from a single build
#All numbers in the log were generated by running that build on a single target
#Therefore:
#Code and data sizes are constant and need only be logged once
#There is no need to disambiguate TEST_CASE_ID w.r.t. toolchain or target

run="$1"
target=
toolchain=
check_dir "${run}"
#check_dir "${run}/.." #would achieve nothing - /.. == /
check_dir "${run}/builds"
for x in `cd "${run}/builds" && ls`; do
  for y in `cd "${run}/builds/${x}" && ls`; do
    if test -e "${run}/builds/${x}/${y}/logs/${x}.${y}.log"; then
      if test x"${target}" = x; then
        target="${x}"
        toolchain="${y}"
      else
        echo "Found more than one log:" >&2
        echo "${run}/builds/${target}/${toolchain}/logs/${target}.${toolchain}.log" >&2
        echo "${run}/builds/${x}/${y}/logs/${x}.${y}.log" >&2
        echo "There may be others, I stop looking as soon as I find too many." >&2
        exit 1
      fi
    fi
  done
done
if test x"${target}" = x; then
  echo "Found no log" >&2
  exit 1
fi

#Attach raw output - do this first so that we have something to debug if
#later code that scrapes the raw output should fail.
pushd . > /dev/null
cd "${run}"/..
ltra RETCODE text/plain
ltra stdout text/plain
ltra stderr text/plain
cd - > /dev/null
cd "${run}"
ltra linarobenchlog text/plain
cd builds
if test -e "${target}/${toolchain}/cert"; then
  for x in `find ${target}/${toolchain}/cert -type f | sort`; do
    ltra "$x" text/plain
  done
fi
for x in `find ${target}/${toolchain}/logs -type f | sort`; do
  ltra "$x" text/plain
done
popd > /dev/null

#slurp results file
line=("")
while IFS='' read -r l; do line=("${line[@]}" "${l}"); done < "${run}/builds/${target}/${toolchain}/logs/${target}.${toolchain}.log"
line_max=$((${#line[@]} - 1))

#Log individual test results
iteration=0
i=0
while test $i -lt ${line_max}; do
  i=$((i+1))
  if header ${line[$i]}; then
    continue
  elif verification ${line[$i]}; then
    if test $i -eq ${line_max}; then
      echo "No verification lines after verification header" >&2
      exit 1
    fi
    while test $i -lt ${line_max} && ! comment ${line[$((i+1))]}; do
      i=$((i+1))
      name="`name ${line[$i]}`"
      verificationCount["${name}"]=$((${verificationCount["${name}"]:-0} + 1))

      #Log sizes off the first verification run, as these should only be constant
      if test ${verificationCount["${name}"]} -eq 1; then
        cs="`code_size ${line[$i]}`"
        ds="`data_size ${line[$i]}`"
        ltc "${name}[code_size]" --result pass --units 'bytes' --measurement "${cs}"
        ltc "${name}[data+bss_size]" --result pass --units 'bytes' --measurement "${ds}"
      fi

      if pass ${line[$i]}; then
        ltc "${name}[verification[${verificationCount[${name}]}]]" --result pass
      else
        ltc "${name}[verification[${verificationCount[${name}]}]]" --result fail
      fi
    done
  elif performance ${line[$i]}; then
    if test $i -eq ${line_max}; then
      echo "No performance lines after performance header" >&2
      exit 1
    fi
    while test $i -lt ${line_max} && ! comment ${line[$((i+1))]}; do
      i=$((i+1))
      name="`name ${line[$i]}`"
      performanceCount["${name}"]=$((${performanceCount["${name}"]:-0} + 1))
      report_measured "${name}" "iteration[${performanceCount["${name}"]}]" ${line[$i]}
    done
  elif median ${line[$i]}; then
    i=$((i+1))
    name="`name ${line[$i]}`"
    medianCount["${name}"]=$((${medianCount["${name}"]:-0} + 1))
    report_measured "${name}" "median[${medianCount[${name}]}]" ${line[$i]}
  fi
done

#Log ProMarks
markslog="${run}/builds/${target}/${toolchain}/logs/${target}.${toolchain}.mark"
if ! test -e "${markslog}"; then
  markslog="${run}/builds/${target}/${toolchain}/logs/${target}.${toolchain}.noncert_mark"
fi
if test -e "${markslog}"; then
  report_marks "${markslog}"
fi

error=0

